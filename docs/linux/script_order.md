# Linux Script order

reference notes for boot order things since we're in hypervisor land
source [here](https://www.golinuxhub.com/2017/12/step-by-step-linux-boot-process-with/), [here](https://trying2adult.com/the-linux-boot-process-verbose/)

## Bios initialization

1. __POST__ is run by bios

- looks for the __MBR__

    - 512 bytes at 1st sector of disk
    - first 446 bytes are code, next 64 is 16x4 partitions
    - last 2 bytes are MBR signiture

## GRUB2

- __Stage 1__

    1. Also known as the primary bootloader, this is a 512 byte image (named “boot.img” ) that is located in the MBR. Its only task is to load stage 1.5. The Logical Block Address (LBA) of the first sector of stage 1.5 (core.img) is hardcoded into boot.img. When boot.img executes, it loads this first sector of core.img into memory and transfers control to it.In a typical Linux system, you can find boot.img is located at /boot/grub2/i386-pc/boot.img.


- __Stage 1.5__

    1. __Core.img__ is the core image of GRUB. It is built dynamically from the kernel image and an arbitrary list of modules by the grub-mkimage program. Usually, it contains enough modules to access /boot/grub, and loads everything else (including menu handling, the ability to load target operating systems, and so on) from the file system at run-time.

    - Once executed, core.img will load its configuration file and any other modules needed, particularly file system drivers at installation time based on the location using which the system is getting booted as explained below
    
    - written between MBR and 1st partition
    
    - first sector is s0 then at s63, there is a __63 sector gap__
    
    - __core.img__ loads filesystem drivers
    
        - __diskboot.img__

            This image is used as the first sector of the core image when booting from a hard disk. It reads the rest of the core image into memory and starts the kernel. Since file system handling is not yet available, it encodes the location of the core image using a block list format.

        - __cdboot.img__

            This image is used as the first sector of the core image when booting from a CD-ROM drive. It performs a similar function to diskboot.img.

        - __pxeboot.img__

            This image is used as the start of the core image when booting from the network using PXE. See Network.

        - __lnxboot.img__

            This image may be placed at the start of the core image in order to make GRUB look enough like a Linux kernel that it can be booted by LILO using an ‘image=’ section.

        - __kernel.img__

            This image contains GRUB’s basic run-time facilities: frameworks for device and file handling, environment variables, the rescue mode command-line parser, and so on. It is rarely used directly, but is built into all core images.

- __Stage 2__

    1. Stage 2 starts when /boot/grub2/grub.cfg is parsed and this is when the GRUB menu is displayed. Stage 2 will load the kernel image (identified by “/boot/vmlinuz-<kernel-version>”) along with the appropriate initramfs images (identified by “/boot/initramfs-<kernel-version>”)NB: GRUB supports chainloading which allows it to boot operating systems that are not directly supported by GRUB, like Windows. When you select a Windows OS, chainloading allows grub to load the Windows bootloader which then continues the boot process to load Windows.

## initrd vs initramfs

The kernel needs to mount the root filesystem which can have one or more of the following properties:

  - Can be on an NFS filesystem.
  - Storage can be using a RAID configuration.
  - May be using LVM.
  - May be encrypted.

The kernel is not compiled with the modules needed to mount a filesystem with any of the properties above. These modules, along with the rest of the modules used by the kernel, instead are stored under “/lib/modules” directory. However, in order for the kernel to access these modules, it needs to mount the root filesystem… this is the chicken and egg problem.

The solution to this is to create a very small filesystem called initramfs. Initramfs has all the necessary drivers and modules that the kernel needs to mount the root filesystem. This filesystem is compressed into an archive which is extracted by the kernel into a temporary filesystem (tmpfs) that’s mounted in memory.

The initramfs image is generated by a program called dracut.
Initrd vs initramfs

Both do the same thing but in slightly different ways.

Initrd is a disk image which is made available as a special block device “/dev/ram”. This block device contains a filesystem that is mounted in memory. The drivers needed to mount this filesystem, which for example could be ext3, must also be compiled into the kernel.

Initramfs is an archive (which may or may not be compressed) that contains the files/directories you find in a typical Linux filesystem. This archive is extracted by the kernel into a tmpfs that in then mounted in memory… so no need to compile any filesystem drivers into the kernel as you would for initrd.

Initrd was deprecated in favour of initramfs which was introduced in Linux kernel version 2.6.13.

## Kernel

When the kernel is loaded into memory by GRUB2, it first initializes and configures the computer’s memory and configures the various hardware attached to the system, including all processors, I/O subsystems, and storage devices.

The kernel then extracts initramfs into a tmpfs that’s mounted in memory and then uses this to mount the root filesystem as read-only (to protect it in case things don’t go well during the rest of the boot process).

After mounting the root filesystem, the system manager is the first program/daemon to be executed and thus will have a PID of 1. In a SysV system, this daemon is called “init” and SystemD systems, this daemon is systemd.

## SystemD

SystemD is a system and service manager that was designed to replace SysV-Init which has the following limitations:

  1. Services are started sequentially even services that do not depend on each other.
    
  - Longer boot times (most because of the point mentioned above)
    
  - No easy and straightforward way to monitor running services.
    
  - Dependencies have to be handled manually so you need very good knowledge of the dependencies involved when you want to modify the runlevel scripts to add a new service.

Every resource that is managed by SystemD is called a unit. A unit (as defined in the man pages) is a plain-text file that stores information about any one of the following:

  1. a service
  - a socket
  - a device
  - a mount point,
  - an automount point
  - a swap file or partition
  - a start-up target
  - a watched file system path
  - a timer controlled and supervised by systemd
  - a resource management slice or a group of externally created processes.

 - systemd target files are located in /etc/systemd/system/ and correspond to init-v run levels

 - Target files are used to group together units needed for that specific target. These units could be services, devices, sockets, etc and these units are defined as dependencies of that target. There are 3 main ways of defining the dependencies of a target and these are:

    - “Wants=” statements inside the target unit files.
    - “Requires=” statements inside the target unit files.
    - Special “.wants” directories associated with each target unit file found under the directory /etc/systemd/system

        |SysV-Init Runlevel	| SystemD Start-up Target |
        | --- | --- |
        |0: Halt or shutdown the system	| poweroff.target |
        |1: Single User mode	| rescue.target |
        |2: Multi-user mode, without networking	| multi-user.target |
        |3: Full multi user mode, with Networking	| multi-user.target |
        |4: Undefined	| multi-user.target |
        |5: Full multi-user mode with networking and graphical desktop. |	graphical.target | 
        |6: Reboot	| reboot.target |

    - When a service has been configured to start at boot, a symbolic link will be created in the “.wants” directory of the corresponding target

    - If you are curious to know which parts of the start-up process are taking long, you can run the following command: `systemd-analyze critical-chain `

## Login

- __/etc/profile__:
system wide env vars, initial path

- __~/.bash_profile__:
if it exists, its executed after /etc/profile on login

- __~/.bash_login__:
executed it no profile exists

- __~/.profile__:
if no bash_profile or bash_login exists, run this

- __~/.bashrc__:
executed on non-interractive shell start

- __~/etc/bashrc__:
system-wide, runs on shell launch

- __~/bash_logout__:
runs on user loging out

## Run Levels

- __0__: Halt or shutdown the system
- __1__: Single user mode
- __2__: Multi-user mode, without networking
- __3__: Full multi user mode, with NFS (typical for servers)
- __4__: Officially not defined; Unused
- __5__: Full multi user with NFS and graphics (typical for desktops)
- __6__: Reboot
- __s__,__S__ or single: Alternate single user mode
- __emergency__: Bypass rc.sysinit (discussed later in this article)
